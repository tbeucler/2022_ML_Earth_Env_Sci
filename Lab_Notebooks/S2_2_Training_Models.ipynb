{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S2_2_Training_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S2_2_Training_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 4 – Training Models**\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td align=middle>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb\"> Open the original notebook <br><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "5Tt5C4PoIRl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin like in the last notebook: importing a few common modules, ensuring MatplotLib plots figures inline and preparing a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so once again we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n",
        "\n",
        "You don't need to worry about understanding everything that is written in this section."
      ],
      "metadata": {
        "id": "8HQ31GpXuKr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_OXSp49IOF2"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will be working with the [*Iris Flower Dataset*](https://en.wikipedia.org/wiki/Iris_flower_data_set), in which the length and width of both the sepals and petals of three types of Iris flowes were recorded. For reference, these are pictures of the three flowers: <br>\n",
        "\n",
        "<center> In order: Iris Setosa,  Iris Versicolor, and Iris Virginica </center>\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/360px-Kosaciec_szczecinkowaty_Iris_setosa.jpg' height=300 >\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/640px-Iris_versicolor_3.jpg' height=300></img>\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/590px-Iris_virginica.jpg' height=300></img>\n",
        "\n",
        "Photo Credits:[Kosaciec szczecinkowaty Iris setosa](https://en.wikipedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg) by [Radomil Binek](https://commons.wikimedia.org/wiki/User:Radomil) licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en); [Blue flag flower close-up (Iris versicolor)](https://en.wikipedia.org/wiki/File:Iris_versicolor_3.jpg)by Danielle Langlois licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en); [image of Iris virginica shrevei](https://en.wikipedia.org/wiki/File:Iris_virginica.jpg) by [Frank Mayfield](https://www.flickr.com/photos/33397993@N05) licensed under [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/deed.en).\n",
        "<br><br>\n",
        "\n",
        "As you can imagine, this dataset is normally used to train *multiclass*/*multinomial* classification algorithms and not *binary* classification algorithms, since there *are* more than 2 classes. \n",
        "\n",
        "\"*Three classes, even!*\" - an observant TA\n",
        "\n",
        "For this exercise, however, we will implement the binary classification algorithm referred to as the *logistic regression* algorithm (also called logit regression)."
      ],
      "metadata": {
        "id": "wKsvLXdmzqD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the Iris Dataset\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Print out some information about the data\n",
        "print(f'Keys in Iris dictionary: \\n{list(iris.keys())}\\n\\n')\n",
        "print(iris.DESCR)\n",
        "\n",
        "# And load the petal lengths and widths as our input data\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "print(iris['data_module'])\n",
        "\n",
        "# The target data labels Setosa as 0, Versicolor as 1, and Virginica as 2. For \n",
        "# this exercise we will be using only the Versicolor and Virgina sets.\n",
        "bin_indices = np.logical_or(y==1,y==2)\n",
        "bin_X = X[bin_indices]\n",
        "bin_y = (y[bin_indices]==2).astype(np.uint8) # convert to binary"
      ],
      "metadata": {
        "id": "emWru72owjEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of binary classification data we can use to train an algorithm.\n",
        "\n",
        "As we saw during our reading, we need to define three things in order to train our algorithm: the type of algorithm we will train, the cost function (which will tell us how close our prediction is to the truth), and a method for updating the parameters in our model according to the value of the cost function (e.g., the gradient descent method). \n",
        "\n",
        "Let's begin by defining the type of algorithm we will use. We will train a logistic regression model to differentiate between two classes. A reminder of how the logistic regression algorithm works is given below.\n",
        "<br><br><br>\n",
        "The logistic regression algorithm will thus take an input $t$ that is a linear combination of the features:\n",
        "\n",
        "<a name=\"logit\"></a>\n",
        "\n",
        "<center> $t_{\\small{n}} = \\beta_{\\small{0}} + \\beta_{\\small{1}} \\cdot X_{1,n} + \\beta_{\\small{2}} \\cdot X_{2,n}$ </center>\n",
        "\n",
        "where \n",
        "* $n$ is the ID of the sample \n",
        "* $X_{\\small{0}}$ represents the petal length\n",
        "* $X_{\\small{1}}$ represents the petal width\n",
        "\n",
        "This input is then fed into the logistic function, $\\sigma$:\n",
        "\\begin{align} \n",
        "\\sigma: t\\mapsto \\dfrac{1}{1+e^ {-t}}\n",
        "\\end{align}\n",
        "\n",
        "Let's plot it below to remember the shape of the function"
      ],
      "metadata": {
        "id": "jvNBaOWZ9fXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.arange(-4,4,.1)\n",
        "def logistic(in_val):\n",
        "    # Return the value of the logistic function\n",
        "    return 1/(1 + np.exp(- in_val))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.axvline(0, c='black', alpha=1)\n",
        "ax.axhline(0, c='black', alpha=1)\n",
        "\n",
        "[ax.axhline(y_val, c='black', alpha=0.5, linestyle='dotted') for y_val in (0.5,1)]\n",
        "\n",
        "plt.autoscale(axis='x', tight=True)\n",
        "\n",
        "ax.plot(t, logistic(t));\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$\\\\sigma\\\\  \\\\left(t\\\\right)$')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "lgt9dI6b9Zwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the logistic function, we define inputs resulting in $\\sigma\\geq.5$ as belonging to the ***one*** class, and any value below that is considered to belong to the ***zero*** class.\n",
        "\n",
        "We now have a function which lets us map the value of the petal length and width to the class to which the observation belongs (i.e., whether the length and width correspond to Iris Versicolor or Iris Virginica). However, there is a parameter vector **$\\theta$** with a number of parameters that we do not have a value for: <br> $\\theta = [ \\beta_{\\small{0}}, \\beta_{\\small{1}}$, $\\beta_{\\small{2}} ]$\n",
        "\n",
        "**Q1) Set up an array of random numbers between 0 and 1 representing the $\\theta$ vector.**\n",
        "\n",
        "Hint:  Use `rnd_gen`! If you're not sure how to use it, consult the `default_rng` documentation [at this link](https://numpy.org/doc/stable/reference/random/generator.html). For instance, you may use the `random` method of `rnd_gen`."
      ],
      "metadata": {
        "id": "0Ll1PKpjxqLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "-Vk05y1C2VBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to determine whether a set of $\\beta$ values is better than the other, we need to quantify well the values are able to predict the class. This is where the cost function comes in.\n",
        "\n",
        "The cost function, $c$, will return a value close to zero when the prediction, $\\hat{p}$, is correct and a large value when it is wrong. In a binary classification problem, we can use the log loss function. For a single prediction and truth value, it is given by:\n",
        "\\begin{align}\n",
        "        \\text{c}(\\hat{p},y) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        -\\log(\\hat{p})& \\text{if}\\; y=1\\\\\n",
        "        -\\log(1-\\hat{p}) & \\text{if}\\; y=0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "\n",
        "However, we want to apply the cost function to an n-dimensional set of predictions and truth values. Thankfully, we can find the average value of the log loss function $J$ for an an-dimensional set of $\\hat{y}$ & $y$ as follows:\n",
        "\n",
        "\\begin{align}\n",
        "        \\text{J}(\\mathbf{\\hat{p}},y) = - \\dfrac{1}{n} \\sum_{i=1}^{n} \n",
        "        \\left[ y_i\\cdot \\log\\left( \\hat{p}_i \\right) \\right] + \n",
        "        \\left[ \\left( 1 - y_i \\right) \\cdot \\log\\left( 1-\\hat{p}_i \\right) \\right]\n",
        "    \\end{align}\n",
        "\n",
        "We now have a formula that can be used to calculate the average cost over the training set of data.\n",
        "\n",
        "**Q2) Define a log_loss function that takes in an arbitrarily large set of prediction and truths**\n",
        "\n",
        "Hint 1: You need to encode the function $J$ above, for which Numpy's functions may be quite convenient (e.g., [`log`](https://numpy.org/doc/stable/reference/generated/numpy.log.html), [`mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), etc.)\n",
        "\n",
        "Hint 2: Asserting the dimensions of the vector is a good way to check that your function is working correctly. [Here's a tutorial on how to use `assert`](https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html#assertions). For instance, to assert that two vectors `X` and `Y` have the same dimension, you may use:\n",
        "```\n",
        "assert X.shape==Y.shape\n",
        "```"
      ],
      "metadata": {
        "id": "s8KM_CeF2Ven"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's code 💻"
      ],
      "metadata": {
        "id": "XBLxwlSWMoo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(p_hat, y, epsilon=1e-7):\n",
        "  # Write your code here. \n",
        "  # We can also run into problems if p_hat = 0, so add an _epsilon_ term\n",
        "  # when evaluating log(p_hat).\n",
        "  log_p_hat = _____( epsilon + _____ )\n",
        "  \n",
        "  # Calculate the value of the cost function (i.e., what's inside the summation)\n",
        "  c = _____________________________\n",
        "\n",
        "  # After calculating c, assert that c has the same shape as p_hat and y\n",
        "  assert __________\n",
        "  assert __________\n",
        "\n",
        "  # Calculate the value of J from c\n",
        "  J = _____\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "H5fDeL36EauO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a way of quantifying how good our predictions are. The final thing needed for us to train our algorithm is figuring out a way to update the parameters in a way that improves the average quality of our predictions. \n",
        "\n",
        "<br><br>**Warning**: we'll go into a bit of math below <br><br>\n",
        "\n",
        "Let's look at the change in a single parameter within $\\theta$: $\\beta_1$ (given $X_{1,i} = X_1$, $\\;\\hat{p}_{i} = \\hat{p}$, $\\;y_{i} = y$). If we want to know what the effect of changing the value of $\\beta_1$ will have on the log loss function we can find this with the partial derivative:\n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1}\n",
        "$</center>\n",
        "\n",
        "This may not seem very helpful by itself - after all, $\\beta_1$ isn't even in the expression of $J$. But if we use the chain rule, we can rewrite the expression as:\n",
        "<center>\n",
        "        $\\dfrac{\\partial J}{\\partial \\hat{p}} \\cdot\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} \\cdot\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1}$\n",
        "</center>\n",
        "\n",
        "We'll spare you the math (feel free to verify it youself, however!):\n",
        "\n",
        "<center>$\\dfrac{\\partial J}{\\partial \\hat{p}} =  \\dfrac{\\hat{p} - y}{\\hat{p}(1-\\hat{p})}, \\quad\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} = \\hat{p} (1-\\hat{p}), \\quad\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1} = X_1 $\n",
        "</center>\n",
        "\n",
        "and thus \n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1} = (\\hat{p} - y) \\cdot X_1\n",
        "$</center>\n",
        "\n",
        "We can calculate the partial derivative for each parameter in $\\theta$ which, as you may have realized, is simply the $\\theta$ gradient of $J$: $\\nabla_{\\theta}(J)$\n",
        "\n",
        "With all of this information, we can now write $\\nabla_{\\theta} J$ in terms of the error, the feature vector, and the number of samples we're training on!\n",
        "\n",
        "<a name=\"grad_eq\"></a>\n",
        "\n",
        "<center>$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\theta^{(k)}}) = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n}{ \\left ( \\hat{p}^{(k)}_{i} - y_{i} \\right ) \\mathbf{X}_{i}}$</center>\n",
        "\n",
        "Note that here $k$ represents the iteration of the parameters we are currently on.\n",
        "\n",
        "We now have a gradient we can calculate and use in the batch gradient descent method! The updated parameters will thus be:\n",
        "\n",
        "<a name=\"grad_descent\"></a>\n",
        "\n",
        "\\begin{align} \n",
        "{\\mathbf{\\theta}^{(k+1)}} = {\\mathbf{\\theta}^{(k)}} - \\eta\\,\\nabla_{\\theta^{(k)}}J(\\theta^{(k)})\n",
        "\\end{align}\n",
        "\n",
        "Where $\\eta$ is the learning rate parameter. It's also worth pointing out that $\\;\\hat{p}^{(k)}_i = \\sigma\\left(\\theta^{(k)}, X_i\\right) $"
      ],
      "metadata": {
        "id": "aO4Bkm1gFV3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to easily calculate the input to the logistic regression, we'll multiply the $\\theta$ vector with the X data, and as we have a non-zero bias  $\\beta_0$ we'd like to have an X matrix whose first column is filled with ones.\n",
        "\n",
        "\\begin{align}\n",
        "    X_{\\small{with\\ bias}} = \\begin{pmatrix}\n",
        "        1 & X_{1,0} & X_{2,0}\\\\\n",
        "        1 & X_{1,1} & X_{2,1}\\\\\n",
        "        &...&\\\\\n",
        "        1 & X_{1,n} & X_{2,n} \n",
        "        \\end{pmatrix}\n",
        "\\end{align}\n",
        "<br>\n",
        "**Q3) Prepare the `X_with_bias` matrix (remember to use the `bin_X` data and not just `X`). Write a function called `predict` that takes in the parameter vector $\\theta$ and the `X_with_bias` matrix and evaluates the logistic function for each of the samples.**\n",
        "\n",
        "Hint 1: You recently learned how to initialize arrays in the `Numpy` notebook [at this link](https://nbviewer.org/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S1_2_Numpy.ipynb). There are many ways to add a columns of 1 to `bin_X`, for instance using [`np.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) or [`np.append`](https://numpy.org/doc/stable/reference/generated/numpy.append.html).\n",
        "\n",
        "Hint 2:  To clarify, the function `predict` calculates $\\hat{p}$ from $\\beta$ and $\\boldsymbol{X}$.\n",
        "\n",
        "Hint 3: In practice, to calculate the logistic function for each sample, you may follow the equations [higher up in the notebook](#logit) and (1) calculate $t$ from $\\beta$ and $\\boldsymbol{X_{\\mathrm{with\\ bias}}}$ before (2) applying the logistic function $\\sigma$ to $t$. "
      ],
      "metadata": {
        "id": "ML4uik7sbdMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the X_with_bias matrix"
      ],
      "metadata": {
        "id": "3b2oOJ5WKn5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your function predict here"
      ],
      "metadata": {
        "id": "tBLryApsbatR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4) Now that you have a `predict` function, write a `gradient_calc` function that calculates the gradient for the logistic function.**\n",
        "\n",
        "Hint 1: You'll have to feed `theta`, `X`, and `y` to the `gradient_calc` function.\n",
        "\n",
        "Hint 2: You can use [this equation](#grad_eq) to calculate the gradient of the cost function."
      ],
      "metadata": {
        "id": "p6cPbu4LvVES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "BtnANN5WvVuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write a function that will train a logistic regression algorithm!\n",
        "\n",
        "Your `logistic_regression` function needs to:\n",
        "* Take in a set of training input/output data, validation input/output data, a number of iterations to train for, a set of initial parameters $\\theta$, and a learning rate $\\eta$\n",
        "* At each iteration:\n",
        " * Generate a set of predictions on the training data. Hint: You may use your function `predict` on inputs `X_train` from the training set.\n",
        " * Calculate and store the loss function for the training data at each iteration. Hint: You may use your function `log_loss` on inputs `X_train` and outputs `y_train` from the training set.\n",
        " * Calculate the gradient. Hint: You may use your function `grad_calc`.\n",
        " * Update the $\\theta$ parameters. Hint: You need to implement [this equation](#grad_descent).\n",
        " * Generate a set of predictions on the validation data using the updated parameters. Hint: You may use your function `predict` on inputs `X_valid` from the validation set. \n",
        " * Calculate and store the loss function for the validation data. Hint: You may use your function `log_loss` on inputs `X_valid` and outputs `y_valid` from the validation set. \n",
        " * Bonus: Calculate and store the accuracy of the model on the training and validation data as a metric!\n",
        "* Return the final set of parameters $\\theta$ & the stored training/validation loss function values (and the accuracy, if you did the bonus)\n",
        "\n",
        "**Q5) Write the `logistic_regression` function**"
      ],
      "metadata": {
        "id": "PU4A5HVKuAGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "HDsR5TxPt-0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¡¡¡Important Note!!!**\n",
        "\n",
        "The notebook assumes that you will return \n",
        "1. a Losses list, where Losses[0] is the training loss and Losses[1] is the validation loss\n",
        "2. a tuple with the 3 final coefficients ($\\beta_0$, $\\beta_1$, $\\beta_2$)\n",
        "\n",
        "The code for visualizing the bonus accuracy is not included - but it should be simple enough to do in a way similar to that which is done with the losses.\n",
        "\n",
        "---------------------"
      ],
      "metadata": {
        "id": "EWMDLk7wFB0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our logistic regression function, we're all set to train our algorithm! Or are we?\n",
        "\n",
        "There's an important data step that we've neglected up to this point - we need to split the data into the train, validation, and test datasets."
      ],
      "metadata": {
        "id": "2ep5FQYBmqG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = rnd_gen.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = bin_y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = bin_y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = bin_y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "CVrXzjYA2iil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready! \n",
        "\n",
        "**Q6) Train your logistic regression algorithm. Use 5000 iterations, $\\eta$=0.1**\n",
        "\n",
        "Hint: It's time to use the `logistic_regression` function you defined in Q5. "
      ],
      "metadata": {
        "id": "33IhRpME8LOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "losses, coeffs = "
      ],
      "metadata": {
        "id": "dWAr0ORYEYi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model did while learning!"
      ],
      "metadata": {
        "id": "e7WHcpPiEcIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce the Loss Function Visualization Graphs\n",
        "fig, ax = plt.subplots(figsize=(18,8))\n",
        "ax.plot(losses[0], color='blue', label='Training', linewidth=3);\n",
        "ax.plot(losses[1], color='orange', label='Validation', linewidth=3);\n",
        "ax.legend();\n",
        "ax.set_ylabel('Log Loss')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_title('Loss Function Graph')\n",
        "ax.autoscale(axis='x', tight=True)\n",
        "fig.tight_layout();\n",
        "\n",
        "# Let's get predictions from our model for the training, validation, and testing\n",
        "# datasets\n",
        "y_hat_train = (predict(X_train, coeffs)>=.5).astype(int)\n",
        "y_hat_valid = (predict(X_valid, coeffs)>=.5).astype(int)\n",
        "y_hat_test = (predict(X_test, coeffs)>=.5).astype(int)\n",
        "\n",
        "y_sets = [ [y_hat_train, y_train],\n",
        "           [y_hat_valid, y_valid],\n",
        "           [y_hat_test, y_test] ]\n",
        "\n",
        "def accuracy_score(y_hat, y):\n",
        "    assert(y_hat.size==y.size)\n",
        "    return (y_hat == y).sum()/y.size\n",
        "\n",
        "[accuracies.append(accuracy_score(y_set[0],y_set[1])) for y_set in y_sets]\n",
        "\n",
        "printout= (f'Training Accuracy:{accuracies[0]:.1%} \\n'\n",
        "           f'Validation Accuracy:{accuracies[1]:.1%} \\n')\n",
        "\n",
        "# Add the testing accuracy only once you're sure that your model works!\n",
        "\n",
        "\n",
        "print(printout)"
      ],
      "metadata": {
        "id": "4wXFzZPjFjOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on training a logistic regression algorithm from scratch! Once you're done with the upcoming environmental science applications notebook, feel free to come back to take a look at the challenges 😀"
      ],
      "metadata": {
        "id": "4zfXs8M8Osie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges\n",
        "\n",
        "* **C1)** Add L2 Regularization to training function \n",
        "\n",
        "* **C2)** Add early stopping to the training algorithm! Stop training when the accuracy is >=90%\n",
        "\n",
        "* **C3)** Implement a softmax regression model (It's multiclass logistic regression 🙂)"
      ],
      "metadata": {
        "id": "VAa4bzT7PHRG"
      }
    }
  ]
}