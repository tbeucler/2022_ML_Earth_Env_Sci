{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S4_2_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRIN5HFk2jWcGoQROzzjKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S4_2_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EYCb6vvFoSlMoAfr_YXSg8UBoMRAF1cpIUTeUFFhBVYsZw?download=1'>\n",
        "<center> Photo Credits: <a href='https://unsplash.com/photos/PizD8punZsw'>Three Assorted-Color Garbage Cans</a> by <a href='https://unsplash.com/@julytheseventifirst'>Hamza Javaid</a> licensed under the <a href='https://unsplash.com/license'>Unsplash License</a> \n",
        "</center>\n",
        "\n",
        "*Sorting takes a lot of effort - is there a way that we can get computers to do it automatically for us when we don't even know where to begin?*"
      ],
      "metadata": {
        "id": "KO7kiUVK5SGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will be used in the lab session for week 4 of the course, covers Chapters 9 of GÃ©ron, and builds on the [notebooks made available on _Github_](https://github.com/ageron/handson-ml2).\n",
        "\n",
        "Need a reminder of last week's labs? Click [_here_](https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/Week_3_Decision_Trees_Random_Forests_SVMs.ipynb) to go to notebook for week 3 of the course."
      ],
      "metadata": {
        "id": "hsXnPPkh5SDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chapter 9 â€“ Clustering**"
      ],
      "metadata": {
        "id": "LQmG3MgLh6fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "l6_AqYXXCY0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20."
      ],
      "metadata": {
        "id": "SjZzFMhZCYCj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOWGsRI22jOT"
      },
      "outputs": [],
      "source": [
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "rnd_seed = 2022\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup"
      ],
      "metadata": {
        "id": "AVIjgpeDDrJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to load the MNIST dataset from OpenML - we won't be loading it as a Pandas dataframe, but will instead use the Dictionary / ndrray representation."
      ],
      "metadata": {
        "id": "vJfxDFQBCeVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the mnist dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "\n",
        "X = mnist['data']\n",
        "y = mnist['target'].astype(np.uint8)"
      ],
      "metadata": {
        "id": "96ed3Hj8ChTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to subsample the digits in the dataset, choosing a random set of digit classes - we won't even know the number of different digits we will chose; it will be somewhere between 4 and 8!"
      ],
      "metadata": {
        "id": "tW68lz26CoI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the random set of digit labels we will use to extract samples from\n",
        "# the MNIST handwritten digit dataset.\n",
        "\n",
        "digits = rnd_gen.choice(np.arange(10), # Digit Possibilities\n",
        "                        int( np.round( rnd_gen.uniform(3.5, 8.5) ) ), # Number of digits to use\n",
        "                        replace = False) # Can't repeat digits "
      ],
      "metadata": {
        "id": "TxjW4hltD0nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will learn on a total of 8000 digits, evenly distributed amongst the randomly selected digits in the dataset. Let's store the number of samples to be taken from each class."
      ],
      "metadata": {
        "id": "n4PrwneRD1me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find a round number of digits to extract for each digit\n",
        "num_samples = np.round(8000/len(digits) + 1 ).astype(int)"
      ],
      "metadata": {
        "id": "TQrrl9j2Iv7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that out of the way, let's generate the dataset!"
      ],
      "metadata": {
        "id": "zbaBrvOIKjyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder Vars\n",
        "sub_X = None\n",
        "sub_y = None\n",
        "\n",
        "# Looping through digit types\n",
        "for digit in digits:\n",
        "  # find indices where target is digit of interest\n",
        "  y_idxs = y==digit\n",
        "  \n",
        "  # rnd_gen.choice chooses n = balanced_size indices from the set of digits \n",
        "  # available. Since we know the truth is an array with the same number of rows\n",
        "  # as the subset, full of the current digit \n",
        "  X_subset = X[y_idxs][rnd_gen.choice(np.arange(y_idxs.sum()),(num_samples,))]\n",
        "  y_subset = np.full(X_subset.shape[0],digit)\n",
        "\n",
        "  if type(sub_X) == type(None):\n",
        "    sub_X = X_subset\n",
        "    sub_y = y_subset\n",
        "  else:\n",
        "    sub_X = np.vstack([sub_X, X_subset])\n",
        "    sub_y = np.hstack((sub_y,y_subset))\n",
        "\n",
        "# Shuffling the dataset, also limitting the number of digits to 8000 so we can't\n",
        "# cheat and tell how many digits there are by looking at the length of the array\n",
        "shuffler = rnd_gen.permutation(len(sub_X))\n",
        "sub_X = sub_X[shuffler][:8000] \n",
        "sub_y = sub_y[shuffler][:8000]"
      ],
      "metadata": {
        "id": "Wt_XQ9FUCnV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of 8000 random samples that we know belong to somewhere between 4 and 8 clusters ðŸ˜ƒ <br> Can we divide them into these groups without knowing the labels beforehand?\n",
        "\n",
        "**Warning**: Don't expect near perfect results this time."
      ],
      "metadata": {
        "id": "R70HPKpVCgaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering with KMeans"
      ],
      "metadata": {
        "id": "yEwKJDqKmQwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to do is to import the KMeans model from scikit learn. Let's go ahead and do so.\n",
        "\n",
        "###**Q1) Import the KMeans model from scikit learn.**\n",
        "\n",
        "*Hint: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for the Kmeans implementation in sklearn.*"
      ],
      "metadata": {
        "id": "eeR0w4WSmV_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here"
      ],
      "metadata": {
        "id": "FVRiu373mWYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't know how many clusters we should use to split the data. Our first instinct would be to use as many clusters as the number of digits we have, but even that is not necessarily optimal. Why dont we try all K's between 4 and 40?\n",
        "\n",
        "To do so, we'll need to begin by training a Kmeans algorithm for each value of K we're interested in. \n"
      ],
      "metadata": {
        "id": "5wuMA-ir7AGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How long does it take to train a single KMeans model with 10 initial centroid settings? This will give us an idea as to whether it may be a good idea to apply a dimensionality reduction algorithm before fitting our models.\n",
        "\n",
        "###**Q2) Import python's time library and measure how long it takes to train a single KMeans model with 3 clusters on the raw data subset.**\n",
        "\n",
        "*Hint 1: [Here is the documetation](https://docs.python.org/3/library/time.html#time.time) for the function used to get timestamps*"
      ],
      "metadata": {
        "id": "KX2Op57zAZ-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ______\n",
        "\n",
        "t0 = _____._____()\n",
        "kmeans_test = KMeans(n_clusters=_____, # Number of clusters to split into \n",
        "                     random_state = _____) # Random seed\n",
        "kmeans_test.fit(_____) # Fitting to data subset\n",
        "t1 = _____._____()\n",
        "\n",
        "print(f\"Training took {(____ - ____):.2f}s\")"
      ],
      "metadata": {
        "id": "GPqc5D1cAY5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should seem like a bit longer than we want. (3.42 seconds during testing of the notebook). Doing this many times means there's a possibility we'll be sitting around doing *nothing*, possiblity for *several minutes*. ***Who has time for this?***\n",
        "\n",
        "Let's reduce the dataset using PCA, capturing 99% of the variability in the data.\n",
        "\n",
        "###**Q3) Import PCA from scikit and reduce the dimensionality of our input data. 99% of the variance in the data should be captured.**\n",
        "\n",
        "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for PCA.*\n",
        "\n",
        "*Hint 2: `.fit_transform()` will be very useful*"
      ],
      "metadata": {
        "id": "7clPTnByCnuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn._____ import _____ # Import PCA\n",
        "\n",
        "pca = _____(_____) # Instantiate PCA, setting it to explain 95% of the variance\n",
        "reduced_X = pca._____(_____) # Transform the data subset"
      ],
      "metadata": {
        "id": "l7gGFSUiDxGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try training a KMeans model on the reduced dataset and see if our training time improved...\n",
        "\n",
        "###**Q5) Repeat Q2 using the reduced dataset**\n",
        "*Hint 1: We're still splitting the data into 3 clusters*"
      ],
      "metadata": {
        "id": "OBUe-CCvGNVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the code\n",
        "t0 = _____._____()\n",
        "kmeans_test = KMeans(n_clusters=_____, # Number of clusters to split into \n",
        "                     random_state = _____) # Random seed\n",
        "kmeans_test.fit(_____) # Fitting to reduced data subset\n",
        "t1 = _____._____()\n",
        "\n",
        "print(f\"Training took {(____ - ____):.2f}s\")"
      ],
      "metadata": {
        "id": "NIAWAWLwGhdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's somewhat better (1.42 seconds during my testing). Let's try training on this reduced dataset!"
      ],
      "metadata": {
        "id": "GnSq_d_PGtek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q6) Train a KMeans model for $\\; 2 \\le k \\le 20$**\n",
        "\n",
        "*Hint 1: Set up a range using python's `range` function*\n",
        "\n",
        "*Hint 2: You can store each trained model by appending it to a list as you iterate*"
      ],
      "metadata": {
        "id": "0H8NeEuXHBNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the code\n",
        "\n",
        "k_list = _____\n",
        "\n",
        "kmeans_models = _____\n",
        "\n",
        "t0 = time.time() # Get a timestamp to keep track of time \n",
        "for k in k_list:\n",
        "    #print out a statement stating which k value you are working on\n",
        "    t1 = time.time() # Get a current timestamp\n",
        "    print(f\"\\r Currently working on k={_____}, elapsed time: {(t1 - t0):.2f}s\", end=\"\")\n",
        "    \n",
        "    kmeans = ________(_____=k, # Set the number of clusters \n",
        "                      _____= rnd_seed ) # Set the random state\n",
        "    kmeans._____(_____) # Fit the model to the reduced data subset\n",
        "\n",
        "    kmeans_models._____(_____) # store the model trained to predict  k clusters\n",
        "\n",
        "print(f\"\\r Finished training the models! It took: {(time.time() - t0):.2f}s\")"
      ],
      "metadata": {
        "id": "iKajWdb760IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should hopefully remember the silhouette score and inertia metrics from the reading. Let's go ahead and make a $k$ vs $silhouette$ plot and a $k$ vs $inertia$ plot. "
      ],
      "metadata": {
        "id": "7yTraW93IF5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q7) Import the silhouette score metric and generate a silhouette score value for each model we trained**\n",
        "\n",
        "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) for the Silhouette score implementation in scikit learn*"
      ],
      "metadata": {
        "id": "I5f4hywgIyFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "from sklearn.________ import ______\n",
        "\n",
        "silhouette_scores = [silhouette_score(_______, model._______)\n",
        "                     for model in _____] "
      ],
      "metadata": {
        "id": "8qmBfe8oIkfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q8) Plot comparing $k$ vs Silhouette Score. Highlight the maximum score**\n",
        "\n",
        "*Hint 1: You'll need to find the position of the best score in the silhouette score list. [Here is the documentation ](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)to a numpy function that would be very useful for this.*\n",
        "\n"
      ],
      "metadata": {
        "id": "pDt_a4PqWQf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "best_index =  # Find the index of the model with the highest score\n",
        "best_k = _____[best_index] # Get the best K value, per the silhouette score\n",
        "best_score = silhouette_scores[best_index]\n",
        "\n",
        "# Make a figure with size (18,6)\n",
        "fig, ax = ___.______()\n",
        "\n",
        "# Make the plot with the K values in the horizontal axis and the silhouette\n",
        "# score in the vertical axis\n",
        "ax.plot(_______, ______, \"bo-\") \n",
        "\n",
        "ax.set_xlabel(\"$k$\", fontsize=14) \n",
        "ax.set_ylabel(\"Silhouette score\", fontsize=14)\n",
        "\n",
        "ax.plot(best_k, best_score, \"rs\")"
      ],
      "metadata": {
        "id": "OkjyAv1jWQme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll plot the value of $k$ vs inertia\n",
        "\n",
        "###**Q9) Plot comparing $k$ vs Inertia. Highlight the inertia for the model with the highest silhouette score**\n",
        "\n",
        "*Hint 1: If you followed the previous step as it was written, you have the index for the best model stored in*`best_index`.\n",
        "\n",
        "*Hint 2: [The KMeans documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) details the attribute in which the model's intertia is stored.*\n"
      ],
      "metadata": {
        "id": "ACdrjMZlz-z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the Code\n",
        "inertias = [model._____ for model in _____] # Iterate through list of kmeans models"
      ],
      "metadata": {
        "id": "gEC3lr000RDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_inertia = # Get the inertia for the model with the highest silhouette score\n",
        "\n",
        "# Make a figure with size (18,6)\n",
        "fig, ax = ___.___()\n",
        "\n",
        "# Make the plot with the K values in the horizontal axis and the inertia in the\n",
        "# vertical axis\n",
        "ax.plot(_______, ______, \"bo-\") \n",
        "\n",
        "ax.set_xlabel(\"$k$\", fontsize=14) \n",
        "ax.set_ylabel(\"Inertia\", fontsize=14)\n",
        "\n",
        "ax.plot(best_k, best_inertia, \"rs\")"
      ],
      "metadata": {
        "id": "LAy0DMLy5KG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you ran the notebook with the default random seed, you may be surprised to see that the best performing KMeans model is the one that breaks it off into two clusters! The next best two will be those associated with $k=4$ and $k=5$. Let's get some plots to try to make sense of the results, since we have the actual labels.\n",
        "\n",
        "There aren't any more questions to answer from here on out, but you *will* have to change the code if you started out from a different random seed! I'll try to be good about pointing out what code you'll need to change.\n",
        "\n",
        "Let's begin by using [scikit's TSNE implementation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to reduce the dimensionality of the dataset for plotting. This will take a bit of computation time!"
      ],
      "metadata": {
        "id": "p2aDRxJj7Pen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, # We'll project onto 2D plane\n",
        "            random_state=rnd_seed, # We need a random seed\n",
        "            learning_rate='auto') #Let the algorithm handle the learning rate\n",
        "\n",
        "# And now get the input data in 2-component reduced form\n",
        "X_plot = tsne.fit_transform(sub_X)"
      ],
      "metadata": {
        "id": "Kr24Ye_T8sDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's continue by making a list of the three best models."
      ],
      "metadata": {
        "id": "c7BlPG2YdQ1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []\n",
        "\n",
        "# We get the best model automagically\n",
        "best_models.append(kmeans_models[best_index]) \n",
        "\n",
        "# But the 2nd and 3rd best are retrieved manually.\n",
        "best_models.append(kmeans_models[2])\n",
        "best_models.append(kmeans_models[3])"
      ],
      "metadata": {
        "id": "wjo_EZs4-kw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, let's get a set of predictions for each model and store it in a list!"
      ],
      "metadata": {
        "id": "l04hAbNHeF7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = []\n",
        "for model in best_models:\n",
        "    pred_labels.append(model.predict(reduced_X))"
      ],
      "metadata": {
        "id": "7CMme4lpeM3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas will make producing a nice plot a lot simpler. Let's import it and make a dataframe with the reduced input components, the truth labels, and the predicted cluster labels. \n",
        "\n",
        "Note that the predicted labels don't correspond to the digit labels since this is an unsupervised model!"
      ],
      "metadata": {
        "id": "PzRQD2xVeVMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "plot_data = np.stack([X_plot[:,0], X_plot[:,1], sub_y, *pred_labels],axis=1)\n",
        "df = pd.DataFrame(plot_data, columns=['X1','X2','truth','pred_1','pred_2','pred_3'])"
      ],
      "metadata": {
        "id": "6vHSkMKSebwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we'll make a nice, big 4x4 plot that allows us to see the true answers and how our algorithm clustered our data!"
      ],
      "metadata": {
        "id": "FW9nDgbxe4ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpl.axes.titlesize = 'larger'\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16,16))\n",
        "\n",
        "groups = df.groupby('truth')\n",
        "for label, group in groups:\n",
        "    axes[0,0].plot(group.X1, group.X2, marker='o', linestyle='', markersize=4, label=int(label))\n",
        "axes[0,0].legend(fontsize=12)\n",
        "axes[0,0].set_title('Truth', fontsize=16)\n",
        "axes[0,0].axis('off')\n",
        "\n",
        "groups = df.groupby('pred_1')\n",
        "for label, group in groups:\n",
        "    axes[0,1].plot(group.X1, group.X2, marker='o', linestyle='', markersize=4)\n",
        "axes[0,1].set_title('\"Best\" Clustering', fontsize=16)\n",
        "axes[0,1].axis('off')\n",
        "\n",
        "groups = df.groupby('pred_2')\n",
        "for label, group in groups:\n",
        "    axes[1,0].plot(group.X1, group.X2, marker='o', linestyle='', markersize=4)\n",
        "axes[1,0].set_title('$2^{nd}$ Best Clustering', fontsize=16)\n",
        "axes[1,0].axis('off')\n",
        "\n",
        "groups = df.groupby('pred_3')\n",
        "for label, group in groups:\n",
        "    axes[1,1].plot(group.X1, group.X2, marker='o', linestyle='', markersize=4)\n",
        "axes[1,1].set_title('$3^{rd}$ Best Clustering', fontsize=16)\n",
        "axes[1,1].axis('off')"
      ],
      "metadata": {
        "id": "P5gFTg7XfCQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming you started out from the intended random seed, you'll be able to see that the \"Best\" model is trying to separate the 0 digits from the non-zero digits. The $2^{nd}$ best model is able to separate the digits pretty well, but lumps 4s and 9s into a single cluster! \n",
        "\n",
        "The $3^{rd}$ best model begins to group digits into clusters that may not have much significance to us at first glance, but whose metrics seem to indicate worse performance and whose results would need further analysis to try to understand."
      ],
      "metadata": {
        "id": "kzx17wP7f6i6"
      }
    }
  ]
}