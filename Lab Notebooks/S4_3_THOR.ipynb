{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S4_3_THOR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSo/5k1rizBeuG6FFpL4vJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab%20Notebooks/S4_3_THOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/ESLP1e1BfUxKu-hchh7wZKcBZiG3bJnNbnt0PDDm3BK-9g?download=1'>\n",
        "\n",
        "<center> \n",
        "Photo Credits: <a href=\"https://unsplash.com/photos/zCMWw56qseM\">Sea Foam</a> by <a href=\"https://unsplash.com/@unstable_affliction\">Ivan Bandura</a> licensed under the <a href='https://unsplash.com/license'>Unsplash License</a> \n",
        "</center>\n",
        "\n",
        "\n",
        ">*A frequently asked question related to this work is ‚ÄúWhich mixing processes matter most for climate?‚Äù As with many alluringly comprehensive sounding questions, the answer is ‚Äúit depends.‚Äù* <br>\n",
        "> $\\qquad$ MacKinnon, Jennifer A., et al. <br>$\\qquad$\"Climate process team on internal wave‚Äìdriven ocean mixing.\" <br>$\\qquad$ Bulletin of the American Meteorological Society 98.11 (2017): 2429-2454."
      ],
      "metadata": {
        "id": "dUurnKLqq1un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In week 4's final notebook, we will perform clustering to identify regimes in data taken from the realistic numerical ocean model [Estimating the Circulation and Climate of the Ocean](https://www.ecco-group.org/products-ECCO-V4r4.htm). Sonnewald et al. point out that finding robust regimes is intractable with a na√Øve approach, so we will be using using reduced dimensionality data. \n",
        "\n",
        "It is worth pointing out, however, that the reduction was done with an equation instead of one of the algorithms we discussed this week. If you're interested in the full details, you can check out [Sonnewald et al. (2019)](https://doi.org/10.1029/2018EA000519)"
      ],
      "metadata": {
        "id": "yT4KFNrpZIfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "PHKbQVLOUC5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ‚â•0.20."
      ],
      "metadata": {
        "id": "4TxieA70pcLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S61_Smb9T6ad"
      },
      "outputs": [],
      "source": [
        "# Python ‚â•3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ‚â•0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import xarray as xr\n",
        "import pooch\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"dim_reduction\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're going to import the [StandardScaler](https://duckduckgo.com/sklearn.preprocessing.standardscaler) function from scikit's preprocessing tools, import the [scikit clustering library](https://duckduckgo.com/sklearn.clustering), and set up the colormap that we will use when plotting."
      ],
      "metadata": {
        "id": "6vUFq1DYUqCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.cluster as cluster\n",
        "\n",
        "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
        "colors = ['royalblue', 'cyan','yellow', 'orange', 'magenta', 'red']\n",
        "mycmap = ListedColormap(colors)"
      ],
      "metadata": {
        "id": "34PAL9TkUHd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "Fr46ZRUjpTzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to do is retrieve the list of files we'll be working on. We'll rely on pooch to access the files hosted on the cloud."
      ],
      "metadata": {
        "id": "O9QOXpKGpgt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the files from the cloud using Pooch.\n",
        "data_url = 'https://unils-my.sharepoint.com/:u:/g/personal/tom_beucler_unil_ch/EUYqUzpIjoJBui02QEo6q1wBSN1Zsi1ofE6I3G4B9LJn_Q?download=1'\n",
        "hash = '3f41661c7a087fa7d7af1d2a8baf95c065468f8a415b8514baedda2f5bc18bb5'\n",
        "\n",
        "files = pooch.retrieve(data_url, known_hash=hash, processor=pooch.Unzip())\n",
        "[print(filename) for filename in files];"
      ],
      "metadata": {
        "id": "nDNJnzOAfKE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now that we have a set of files to load, let's set up a dictionary with the variable names as keys and the data in numpy array format as the values."
      ],
      "metadata": {
        "id": "bZxAvW1gBbO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read in the variable names from the filepaths\n",
        "var_names = []\n",
        "[var_names.append(path.split('/')[-1][:-4]) for path in files]\n",
        "\n",
        "# And build a dictionary of the data variables keyed to the filenames\n",
        "data_dict = {}\n",
        "for idx, val in enumerate(var_names):\n",
        "    data_dict[val] = np.load(files[idx]).T\n",
        "\n",
        "#We'll print the name of the variable loaded and the associated shape \n",
        "[print(f'Varname: {item[0]:<15} Shape: {item[1].shape}') for item in data_dict.items()];"
      ],
      "metadata": {
        "id": "b8OmoBGHBkwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a dictionary that uses the filename as the key! Feel free to explore the data (e.g., loading the keys, checking the shape of the arrays, plotting)"
      ],
      "metadata": {
        "id": "RMbhwWTunLvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feel free to explore the data dictionary"
      ],
      "metadata": {
        "id": "rG2pgeH5pB4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're eventually going to have an array of cluster classes that we're going to use to label dynamic regimes in the ocean. Let's make an array full of NaN (not-a-number) values that has the same shape as our other variables and store it in the data dictionary. "
      ],
      "metadata": {
        "id": "xXwnzzbRtTYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict['clusters'] = np.full_like(data_dict['BPT'],np.nan)"
      ],
      "metadata": {
        "id": "4VYRgr0cuHKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reformatting as Xarray"
      ],
      "metadata": {
        "id": "pCumDPFEpFcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the original paper, this data was loaded as numpy arrays. However, we'll take this opportunity to demonstrate the same procedure while relying on xarray. First, let's instantiate a blank dataset.<br><br>\n",
        "\n",
        "###**Q1) Make a blank xarray dataset.**<br>\n",
        "*Hint: Look at the xarray [documentation](https://duckduckgo.com/?q=xarray+dataset)*"
      ],
      "metadata": {
        "id": "vz_vQOCguax0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make your blank dataset here! Instantiate the class without passing any parameters."
      ],
      "metadata": {
        "id": "JIPsLT6g-INq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EZv_qqVz_h1Hio6Nq11ckScBb01bGb9jtNKzdqAg1TPrKQ?download=1'>\n",
        "<center> Image taken from the xarray <a href='https://xarray.pydata.org/en/stable/user-guide/data-structures.html#:~:text=Dataset-,xarray.,from%20the%20netCDF%20file%20format.'> <i>Data Structure documentation</i> </a> </center>\n",
        "\n",
        "In order to build the dataset, we're going to need a set of coordinate vectors that help us map out our data! For our data, we have two axes corresponding to longitude ($\\lambda$) and latitude ($\\phi$). \n",
        "\n",
        "We don't know much about how many lat/lon points we have, so let's explore one of the variables to make sense of the data the shape of one of the numpy arrays.\n",
        "\n",
        "###**Q2) Visualize the data using a plot and printing the shape of the data to the console output.**"
      ],
      "metadata": {
        "id": "jJK9Ud9tp3_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the code\n",
        "# Let's print out an image of the Bottom Pressure Torques (BPT)\n",
        "plt.imshow( ________ , origin='lower')\n",
        "\n",
        "# It will also be useful to store and print out the shape of the data\n",
        "data_shape = _________.shape\n",
        "print(data_shape)"
      ],
      "metadata": {
        "id": "Kiw2tAT1D6Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how the resolution of our data, we can prepare a set of axis arrays. We will use these to organize the data we will feed into the dataset.\n",
        "\n",
        "###**Q3) Prepare the latitude and longitude arrays to be used as axes for our dataset**\n",
        "\n",
        "*Hint 1: You can build ordered numpy arrays using, e.g., [numpy.linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) and [numpy.arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)*\n",
        "\n",
        "*Hint 2: You can rely on the data_shape variable we loaded previously to know how many points you need along each axis*"
      ],
      "metadata": {
        "id": "aXyxepjm-N5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the code\n",
        "# Let's prepare the lat and lon axes for our data.\n",
        "lat = \n",
        "lon = "
      ],
      "metadata": {
        "id": "vwzO0ZWMNHKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the axes we need, we can build xarray [*data arrays*](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.html) for each data variable. Since we'll be doing it several times, let's go ahead and defined a function that does this for us!\n",
        "\n",
        "###**Q4) Define a function that takes in: 1) an array name, 2) a numpy array, 3) a lat vector, and 4) a lon vector. The function should return a dataArray with lat-lon as the coordinate dimensions**"
      ],
      "metadata": {
        "id": "yxnAU_gjOqVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the code\n",
        "def np_to_xr(array_name, array, lat, lon):\n",
        "    #building the xarrray\n",
        "    da = xr.DataArray(data = ______, # Data to be stored\n",
        "                  \n",
        "                  #set the name of dimensions for the dataArray \n",
        "                  dims = ['lat', '___'],\n",
        "                  \n",
        "                  #Set the dictionary pointing the name dimensions to np arrays \n",
        "                  coords = {'lat':___,\n",
        "                            '___':___},\n",
        "                      \n",
        "                  name=______)\n",
        "    return ______"
      ],
      "metadata": {
        "id": "7FCXLhYfPiQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to build our data array! Let's iterate through the items and merge our blank dataset with the data arrays we create.\n",
        "\n",
        "###**Q5) Build the dataset from the data dictionary**\n",
        "\n",
        "*Hint: We'll be using the xarray merge command to put everything together.*"
      ],
      "metadata": {
        "id": "l9xgQaM9Rrv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code in the notebook assumes you named your dataset ds. Change it to \n",
        "# whatever you used!\n",
        "\n",
        "# Complete the code\n",
        "for key, item in data_dict.items():\n",
        "    # Let's make use of our np_to_xr function to get the data as a dataArray\n",
        "    da = np_to_xr(key, item, lat, lon)\n",
        "\n",
        "    # Merge the dataSet with the dataArray here!\n",
        "    ds = xr.merge( [____ , ____ ] )"
      ],
      "metadata": {
        "id": "4pLXUfvLSZKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You should now have a nicely set up xarray dataset. This let's you access a ton of nice features, e.g.:\n",
        "> Data plotting by calling, e.g., `ds.BPT.plot.imshow(cmap='ocean')`\n",
        "> \n",
        "> Find statistical measures of all variables at once! (e.g.: `ds.std()`, `ds.mean()`)"
      ],
      "metadata": {
        "id": "aC1Gq_q7WEWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play around with the dataset here if you'd like :)"
      ],
      "metadata": {
        "id": "uag0JpWzaaS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to find clusters of data considering each grid point as a datapoint with 5 dimensional data. However, we went through a lot of work to get the data nicely associated with a lat and lon - do we really want to undo that?\n",
        "\n",
        "Luckily, xarray develops foresaw the need to group dimensions together. Let's create a 'flat' version of our dataset using the [`stack`](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.stack.html) method. Let's make a flattened version of our dataset.\n",
        "\n",
        "###**Q6) Store a flattened version of our dataset**\n",
        "\n",
        "*Hint 1: You'll need to pass a dictionary with the 'new' stacked dimension name as the key and the 'flattened' dimensions as the values.*\n",
        "\n",
        "*Hint 2: xarrays have a ['.values' attribute](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.values.html) that return their data as a numpy array.*"
      ],
      "metadata": {
        "id": "vEfuD2O8aZ4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "# Let's store the stacked version of our dataset\n",
        "stacked = ds.stack( { ____ :[ ___ , ___ ] } )\n",
        "\n",
        "# And verify the shape of our data\n",
        "print(stacked.to_array()._____._____)"
      ],
      "metadata": {
        "id": "G4e2NDipb0Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we've ignored an important point - we're supposed to have 5 variables, not 6! As you may have guessed, noiseMask helps us throw away data we dont want (e.g., from land mass or bad pixels). \n",
        "\n",
        "We're now going to clean up the stacked dataset using the noise mask. Relax and read through the code, since there won't be a question in this part :) "
      ],
      "metadata": {
        "id": "MZGPnmooffKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's redefine stacked as all the points where noiseMask = 1, since noisemask\n",
        "# is binary data.\n",
        "\n",
        "print(f'Dataset shape before processing: {stacked.to_array().values.shape}')\n",
        "\n",
        "print(\"Let's do some data cleaning!\")\n",
        "print(f'Points before cleaning: {len(stacked.BPT)}')\n",
        "stacked = stacked.where(stacked.noiseMask==1, drop=True)\n",
        "print(f'Points after cleaning: {len(stacked.BPT)}')\n",
        "\n",
        "# We also no longer need the noiseMask variable, so we can just drop it.\n",
        "\n",
        "print('And drop the noisemask variable...')\n",
        "print(f'Before dropping: {stacked.to_array().values.shape}')\n",
        "stacked = stacked.drop('noiseMask')\n",
        "print(f'Dataset shape after processing: {stacked.to_array().values.shape}')"
      ],
      "metadata": {
        "id": "sbXscaE_fcOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have several thousand points which we want to divide into clusters using the kmeans clustering algorithm (you can check out the documentation for scikit's implementation of kmeans [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)).\n",
        "\n",
        "You'll note that the algorithm expects the input data `X` to be fed as `(n_samples, n_features)`. This is the opposite of what we have! Let's go ahead and make a copy to a numpy array has the axes in the right order.\n",
        "\n",
        "You'll need xarray's [`.to_array()`](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_array.html) method and [`.values`](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.values.html) parameter, as well as numpy's [`.moveaxis`](https://numpy.org/doc/stable/reference/generated/numpy.moveaxis.html) method.\n",
        "\n",
        "###**Q7) Load the datapoints into a numpy array following the convention where the 0th axis corresponds to the samples and the 1st axis corresponds to the features.** "
      ],
      "metadata": {
        "id": "5Pol90vsU_wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "input_data = np.moveaxis(stacked._____()._____,  # data to reshape\n",
        "                         'number',          # source axis as integer, \n",
        "                         'number')          # destination axis as integer\n",
        "\n",
        "# Does the input data look the way it's supposed to? Print the shape.\n",
        "print(________)"
      ],
      "metadata": {
        "id": "uu0AVVkPeTzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In previous classes we discussed the importance of the scaling the data before implementing our algorithms. Now that our data is all but ready to be fed into an algorithm, let's make sure that it's been scaled.\n",
        "\n",
        "###**Q8) Scale the input data**\n",
        "\n",
        "*Hint 1: Import the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class from scikit and instantiate*\n",
        "\n",
        "*Hint 2: Update the input array to the one return by the [`.fit_transform(X)`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform) method*"
      ],
      "metadata": {
        "id": "AoZSkMieiL9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your scaling code here"
      ],
      "metadata": {
        "id": "D2sZPWf1ignO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're finally ready to train our algorithm! Let's load up the kmeans model and find clusters in our data.\n",
        "\n",
        "###**Q9) Instantiate the kmeans clustering algorithm, and then fit it using 50 clusters, trying out 10 different initial centroids.**\n",
        "\n",
        "*Hint 1: `sklearn.cluster` was imported during the notebook setup! [Here is the scikit `Kmeans` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).*\n",
        "\n",
        "*Hint 2: Use the `fit_predict` method to organize the data into clusters*\n",
        "\n",
        "*Warning! : Fitting the data may take some time (under a minute during the testing of the notebook)"
      ],
      "metadata": {
        "id": "_z3e6dU8rInv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "kmeans = cluster._____(________ =50,  # Number of clusters\n",
        "                        ________ =42, # setting a random state\n",
        "                        ________ =10, # Number of initial centroid states to try\n",
        "                       verbose = 1)   # Verbosity so we know things are working\n",
        "\n",
        "cluster_labels = kmeans.______(____) # Feed in out scaled input data!"
      ],
      "metadata": {
        "id": "DtYyiksRrHPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of cluster labels that group the data into 50 similar groups. Let's store it in our stacked dataset!"
      ],
      "metadata": {
        "id": "XZXYUggc8xex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's run this line\n",
        "stacked['clusters'].values = cluster_labels"
      ],
      "metadata": {
        "id": "aH0qNHM89GER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of labels, but they're stored in a flattened array. Since we'd like to see the data as a map, we still have some work to do. Let's go back to a 2D representation of our values.\n",
        "\n",
        "###**Q10) Turn the flattened xarray back into a set of 2D fields**\n",
        "*Hint 1: xarrays have an [`.unstack` method](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.unstack.html) that you will find to be very useful for this."
      ],
      "metadata": {
        "id": "pFPN00oH9oI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code:\n",
        "processed_ds = ds.____()"
      ],
      "metadata": {
        "id": "lp1u28inKUNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have an unstacked dataset, and can now easily plot out the clusters we found!\n",
        "\n",
        "###**Q11) Plot the 'cluster' variable using the buil-in xarray function**\n",
        "*Hint: `.plot()` [link text](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.plot.html) let's you access the xarray implementations of [`pcolormesh`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.pcolormesh.html) and [`imshow`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html).*"
      ],
      "metadata": {
        "id": "y4w0zUDn8OOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DLLCcgw_n3tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare your results to those from the paper:\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EdLh6Ds0yVlFivyfIOXlV74B_G35dVz87GFagzylIG-gZA?download=1'>"
      ],
      "metadata": {
        "id": "sOja6JCxo45g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to find the 5 most common regimes, and group the rest. This isn't straightforward, so we've gone ahead and prepared the code for you. Run through it and try to understand what the code is doing!"
      ],
      "metadata": {
        "id": "HrLKLVWXtgeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_ds['final_clusters'] = (processed_ds.clusters * 0) - 1\n",
        "\n",
        "# Find the 5 most common cluster labels\n",
        "top_clusters = processed_ds.groupby('clusters').count().sortby('BPT').tail(5).clusters.values\n",
        "\n",
        "#Build the set of indices for the cluster data, used for rewriting cluster labels\n",
        "for idx, label in enumerate(top_clusters):\n",
        "    #Find the indices where the label is found\n",
        "    indices = (processed_ds.clusters == label)\n",
        "\n",
        "    processed_ds['final_clusters'].values[indices] = 4-idx\n",
        "\n",
        "# Set the remaining unlabeled regions to category 5 \"non-linear\"\n",
        "processed_ds['final_clusters'].values[processed_ds.final_clusters==-1] = 5\n",
        "\n",
        "# Plot the figure\n",
        "processed_ds.final_clusters.plot.imshow(cmap=mycmap, figsize=(18,8));"
      ],
      "metadata": {
        "id": "UO5OohbKptau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare it to the regimes found in the paper:\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EehuR9cUfaJImrw4DCAzDPoBiGuG7R3Ys6453Umi1cN_OQ?download=1'>\n",
        "\n"
      ],
      "metadata": {
        "id": "eEMmkpGkc1Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors then went on to train neural networks ***to infer in-depth dynamics from data that is largely readily available from for example CMIP6 models, using NN methods to infer the source of predictive skill*** and ***to apply the trained Ensemble MLP to a climate model in order to assess circulation changes under global heating***. \n",
        "\n",
        "For our purposes, however, we will say goodbye to *THOR* at this point üòÉ"
      ],
      "metadata": {
        "id": "nbWgoD7bv8AH"
      }
    }
  ]
}